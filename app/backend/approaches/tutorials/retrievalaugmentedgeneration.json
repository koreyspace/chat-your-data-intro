{
    "Teach me about retrieval augmented generation": {
        "response": "Retrieval-augmented generation (RAG) is a method that combines pre-trained parametric and non-parametric memory for language generation. The parametric memory is a pre-trained Seq2Seq model, while the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. This method has been explored for a variety of knowledge-intensive NLP tasks, and in language generation tasks, RAG models have been found to generate more specific, diverse, and factual language compared to a state-of-the-art parametric-only Seq2Seq baseline [Retrieval-Augmented-Generation.pdf#page=1].\\n\\nFurthermore, RAG techniques can also use a method called Zero-Shot Retrieval Augmentation. This method keeps the original large language model's (LLM) architecture and weights unchanged, and uses other techniques such as BM25, nearest neighbors, or frozen pre-trained models like Bert as a retriever. The information retrieved by these methods is then provided as input to the LLM for response generation, improving its performance. In some cases, multiple retrieval iterations are required to complete a task [large-language-model-overview.pdf#page=19].\\n\\nA unique aspect of the RAG approach is that its memory is both human-readable, providing a degree of interpretability, and human-writable, allowing for dynamic updates to the model's memory by editing the document index. This approach has been used in knowledge-intensive dialog, where generators have been directly conditioned on retrieved text [Retrieval-Augmented-Generation.pdf#page=9].",
        "followUpQuestions": [
            "What is the difference between parametric and non-parametric memory?",
            "How does retrieval augmentation work?",
            "What is a Seq2Seq model?"
        ],
        "imagePath": "images/rag-architecture.png"
    },
    "What is the difference between parametric and non-parametric memory?": {
        "response": "Parametric memory and non-parametric memory in the context of RAG models refer to two different ways of storing and retrieving information.\\n\\nParametric memory refers to the information that is stored within the parameters of a pre-trained model. In the case of RAG, the parametric memory is a pre-trained Seq2Seq (sequence-to-sequence) model such as BART. This kind of memory is learned during the training phase and is used to generate responses based on the input. For example, the BART model has been found to store specific knowledge in its parameters, which can assist in generating accurate and relevant responses [Retrieval-Augmented-Generation.pdf#page=6].\\n\\nNon-parametric memory, on the other hand, is a dense vector index, typically of a large document set like Wikipedia. It is accessed with a pre-trained neural retriever (e.g. Dense Passage Retriever). The non-parametric memory in RAG does not consist of trainable parameters, but of vectors that represent the content of the documents in the index. In the case of RAG, this is a vector index of 21M 728-dimensional vectors, consisting of 15.3B values. It serves as an external memory source that the model can use to retrieve relevant information when generating responses [Retrieval-Augmented-Generation.pdf#page=19].\\n\\nThe hybrid approach of RAG combines these two types of memory, allowing the model to generate more specific, diverse and factual responses. The non-parametric component helps guide the generation, drawing out specific knowledge stored in the parametric memory [Retrieval-Augmented-Generation.pdf#page=2][Retrieval-Augmented-Generation.pdf#page=6].",
        "followUpQuestions": [
            "Can Azure Cognitive Search outperform baseline retrieval methods?",
            "How does a RAG model retrieve information from its non-parametric memory?",
            "What is the role of the Dense Passage Retriever in RAG?"
        ]
    },
    "Can Azure Cognitive Search outperform baseline retrieval methods?": {
        "response": "Yes, Azure Cognitive Search can outperform baseline retrieval methods in retrieval-augmented generation (RAG) applications. Azure Cognitive Search supports vector search, a common practice for implementing the retrieval step in RAG applications, which finds relevant passages using semantic similarity. Moreover, it offers additional capabilities that complement and build on vector search, delivering improved relevance. This is particularly true for Generative AI scenarios where applications use the RAG pattern, but these conclusions apply to many general search use cases as well [outperforming-vector-search.pdf#page=1].\\n\\nThe technology behind Azure Cognitive Search includes two main layers of execution: retrieval and ranking. The retrieval layer, often called L1, aims to quickly find all the documents from the index that satisfy the search criteria, which could be millions or billions of documents. These are scored to select the top few to return to the user or to feed to the next layer. Azure Cognitive Search supports three different L1 modes: Keyword and Vector. The Vector mode converts documents from text to vector representations using an embedding model. Retrieval is performed by generating a query embedding and finding the documents whose vectors are closest to the query's [outperforming-vector-search.pdf#page=1].\\n\\nTo assess which retrieval systems and configurations performed the best, Azure conducted experiments by replaying a list of queries against several document indexes for each configuration and produced scores of how good the retrieval and ranking was. The document sets were consistent and sourced from either Azure customers (with their permission) or publicly available benchmarks [outperforming-vector-search.pdf#page=4].",
        "followUpQuestions": [
            "What is the ranking layer in Azure Cognitive Search?",
            "What capabilities does Azure Cognitive Search add to vector search?",
            "How do the Keyword and Vector modes in Azure Cognitive Search work?"
        ]
    }
}